[2022-01-14 15:06:58] mrx_link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8891/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"from sklearn.datasets import load_iris\\nimport pandas as pd\\n\\niris = load_iris()\\n\\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\\ndf[\'class\'] = iris.target\\n\\ndf","metadata":{"componentType":"CodeCell","preDefined":null,"headerColor":0,"comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"DataLoader","preDefined":{"batch_drop":true,"batch_shuffle":true,"batch_size":"4","input_df":"df","outputVar1":"train","outputVar2":"valid","ratio":0.8,"split_shuffle":true,"target_columns":["class"]},"headerColor":0,"comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","preDefined":null,"headerColor":0,"comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"Trainer","preDefined":{"earlyStopping":[{"key":"Monitoring","type":"min","value":"CrossEntropyLoss"}],"inputSetting":{"numberOfEpochs":"n_epochs","trainingDataLoader":"train","unTrainedModel":"model","validationDataLoader":"valid"},"metricButtonToggle":1,"metrics":[{"metric":"CrossEntropyLoss","type":"training"},{"metric":"CrossEntropyLoss","type":"validation"}],"optimizerSetting":{"optimizer":"Adam","settings":[{"key":"Learning Rate","value":"learning_rate"},{"key":"Loss","value":"CrossEntropyLoss"}]},"outputSetting":{"bestModel":"a","lastModel":"b","metricHistory":"c"}},"headerColor":0,"comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"}]}\n', cookie='_xsrf=2|b43ad1e5|2545a1037cca6090ad4963c72e90c037|1640567026', token='a78fe6efaa2d4196c01de5e40d11ab06e53718cf03571c04')
[2022-01-14 15:06:58] mrx_link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': "from sklearn.datasets import load_iris\nimport pandas as pd\n\niris = load_iris()\n\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\ndf['class'] = iris.target\n\ndf", 'metadata': {'componentType': 'CodeCell', 'preDefined': None, 'headerColor': 0, 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'DataLoader', 'preDefined': {'batch_drop': True, 'batch_shuffle': True, 'batch_size': '4', 'input_df': 'df', 'outputVar1': 'train', 'outputVar2': 'valid', 'ratio': 0.8, 'split_shuffle': True, 'target_columns': ['class']}, 'headerColor': 0, 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'preDefined': None, 'headerColor': 0, 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'Trainer', 'preDefined': {'earlyStopping': [{'key': 'Monitoring', 'type': 'min', 'value': 'CrossEntropyLoss'}], 'inputSetting': {'numberOfEpochs': 'n_epochs', 'trainingDataLoader': 'train', 'unTrainedModel': 'model', 'validationDataLoader': 'valid'}, 'metricButtonToggle': 1, 'metrics': [{'metric': 'CrossEntropyLoss', 'type': 'training'}, {'metric': 'CrossEntropyLoss', 'type': 'validation'}], 'optimizerSetting': {'optimizer': 'Adam', 'settings': [{'key': 'Learning Rate', 'value': 'learning_rate'}, {'key': 'Loss', 'value': 'CrossEntropyLoss'}]}, 'outputSetting': {'bestModel': 'a', 'lastModel': 'b', 'metricHistory': 'c'}}, 'headerColor': 0, 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}]}
[2022-01-14 15:06:59] mrx_link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8891/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"from sklearn.datasets import load_iris\\nimport pandas as pd\\n\\niris = load_iris()\\n\\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\\ndf[\'class\'] = iris.target\\n\\ndf","metadata":{"componentType":"CodeCell","preDefined":null,"headerColor":0,"comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"DataLoader","preDefined":{"batch_drop":true,"batch_shuffle":true,"batch_size":"4","input_df":"df","outputVar1":"train","outputVar2":"valid","ratio":0.8,"split_shuffle":true,"target_columns":["class"]},"headerColor":0,"comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","preDefined":null,"headerColor":0,"comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"Trainer","preDefined":{"earlyStopping":[{"key":"Monitoring","type":"min","value":"CrossEntropyLoss"}],"inputSetting":{"numberOfEpochs":"n_epochs","trainingDataLoader":"train","unTrainedModel":"model","validationDataLoader":"valid"},"metricButtonToggle":1,"metrics":[{"metric":"CrossEntropyLoss","type":"training"},{"metric":"CrossEntropyLoss","type":"validation"}],"optimizerSetting":{"optimizer":"Adam","settings":[{"key":"Learning Rate","value":"learning_rate"},{"key":"Loss","value":"CrossEntropyLoss"}]},"outputSetting":{"bestModel":"a","lastModel":"b","metricHistory":"c"}},"headerColor":0,"comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"}]}\n', cookie='_xsrf=2|b43ad1e5|2545a1037cca6090ad4963c72e90c037|1640567026', token='a78fe6efaa2d4196c01de5e40d11ab06e53718cf03571c04')
[2022-01-14 15:06:59] mrx_link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': "from sklearn.datasets import load_iris\nimport pandas as pd\n\niris = load_iris()\n\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\ndf['class'] = iris.target\n\ndf", 'metadata': {'componentType': 'CodeCell', 'preDefined': None, 'headerColor': 0, 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'DataLoader', 'preDefined': {'batch_drop': True, 'batch_shuffle': True, 'batch_size': '4', 'input_df': 'df', 'outputVar1': 'train', 'outputVar2': 'valid', 'ratio': 0.8, 'split_shuffle': True, 'target_columns': ['class']}, 'headerColor': 0, 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'preDefined': None, 'headerColor': 0, 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'Trainer', 'preDefined': {'earlyStopping': [{'key': 'Monitoring', 'type': 'min', 'value': 'CrossEntropyLoss'}], 'inputSetting': {'numberOfEpochs': 'n_epochs', 'trainingDataLoader': 'train', 'unTrainedModel': 'model', 'validationDataLoader': 'valid'}, 'metricButtonToggle': 1, 'metrics': [{'metric': 'CrossEntropyLoss', 'type': 'training'}, {'metric': 'CrossEntropyLoss', 'type': 'validation'}], 'optimizerSetting': {'optimizer': 'Adam', 'settings': [{'key': 'Learning Rate', 'value': 'learning_rate'}, {'key': 'Loss', 'value': 'CrossEntropyLoss'}]}, 'outputSetting': {'bestModel': 'a', 'lastModel': 'b', 'metricHistory': 'c'}}, 'headerColor': 0, 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}]}
[2022-01-14 15:06:59] mrx_link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8891/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"from sklearn.datasets import load_iris\\nimport pandas as pd\\n\\niris = load_iris()\\n\\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\\ndf[\'class\'] = iris.target\\n\\ndf","metadata":{"componentType":"CodeCell","preDefined":null,"headerColor":0,"comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"DataLoader","preDefined":{"batch_drop":true,"batch_shuffle":true,"batch_size":"4","input_df":"df","outputVar1":"train","outputVar2":"valid","ratio":0.8,"split_shuffle":true,"target_columns":["class"]},"headerColor":0,"comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","preDefined":null,"headerColor":0,"comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"Trainer","preDefined":{"earlyStopping":[{"key":"Monitoring","type":"min","value":"CrossEntropyLoss"}],"inputSetting":{"numberOfEpochs":"n_epochs","trainingDataLoader":"train","unTrainedModel":"model","validationDataLoader":"valid"},"metricButtonToggle":1,"metrics":[{"metric":"CrossEntropyLoss","type":"training"},{"metric":"CrossEntropyLoss","type":"validation"}],"optimizerSetting":{"optimizer":"Adam","settings":[{"key":"Learning Rate","value":"learning_rate"},{"key":"Loss","value":"CrossEntropyLoss"}]},"outputSetting":{"bestModel":"a","lastModel":"b","metricHistory":"c"}},"headerColor":0,"comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"}]}\n', cookie='_xsrf=2|b43ad1e5|2545a1037cca6090ad4963c72e90c037|1640567026', token='a78fe6efaa2d4196c01de5e40d11ab06e53718cf03571c04')
[2022-01-14 15:06:59] mrx_link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': "from sklearn.datasets import load_iris\nimport pandas as pd\n\niris = load_iris()\n\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\ndf['class'] = iris.target\n\ndf", 'metadata': {'componentType': 'CodeCell', 'preDefined': None, 'headerColor': 0, 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'DataLoader', 'preDefined': {'batch_drop': True, 'batch_shuffle': True, 'batch_size': '4', 'input_df': 'df', 'outputVar1': 'train', 'outputVar2': 'valid', 'ratio': 0.8, 'split_shuffle': True, 'target_columns': ['class']}, 'headerColor': 0, 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'preDefined': None, 'headerColor': 0, 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'Trainer', 'preDefined': {'earlyStopping': [{'key': 'Monitoring', 'type': 'min', 'value': 'CrossEntropyLoss'}], 'inputSetting': {'numberOfEpochs': 'n_epochs', 'trainingDataLoader': 'train', 'unTrainedModel': 'model', 'validationDataLoader': 'valid'}, 'metricButtonToggle': 1, 'metrics': [{'metric': 'CrossEntropyLoss', 'type': 'training'}, {'metric': 'CrossEntropyLoss', 'type': 'validation'}], 'optimizerSetting': {'optimizer': 'Adam', 'settings': [{'key': 'Learning Rate', 'value': 'learning_rate'}, {'key': 'Loss', 'value': 'CrossEntropyLoss'}]}, 'outputSetting': {'bestModel': 'a', 'lastModel': 'b', 'metricHistory': 'c'}}, 'headerColor': 0, 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}]}
[2022-01-14 15:10:46] mrx_link.MRXLinkMagics.mrxlink_execute_node() DEBUG: args: Namespace(cell='from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()\n', id='62535059-9784-42d0-9799-29d5c99f3472', name='Unknown', type='CodeCell')
[2022-01-14 15:10:48] mrx_link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component Modeling (62535059-9784-42d0-9799-29d5c99f3472) status invalid -> running
[2022-01-14 15:10:48] mrx_link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: 
def func62535059_9784_42d0_9799_29d5c99f3472(_kwargs):
    # BEGIN module imports

    # END module imports

    global _link_stdin_lock
    # BEGIN parents locals
    n_epochs = _kwargs['n_epochs']
    learning_rate = _kwargs['learning_rate']
    # END parents locals

    # BEGIN code cell

    from torch import nn
    from torch.nn import functional as F
    
    
    class Model(nn.Module):
        def __init__(self, input_dim):
            super(Model, self).__init__()
            self.layer1 = nn.Linear(input_dim, 50)
            self.layer2 = nn.Linear(50, 50)
            self.layer3 = nn.Linear(50, 3)
    
        def forward(self, x):
            x = F.relu(self.layer1(x))
            x = F.relu(self.layer2(x))
            x = F.softmax(self.layer3(x), dim=1)
            return x
    
    
    model = Model(4).double()
    

    # END code cell
    del _kwargs
    return locals()
[2022-01-14 15:10:50] mrx_link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: 62535059-9784-42d0-9799-29d5c99f3472 Not cached
[2022-01-14 15:10:50] mrx_link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component Modeling (62535059-9784-42d0-9799-29d5c99f3472) status running -> successful
[2022-01-14 15:10:50] mrx_link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Modeling (62535059-9784-42d0-9799-29d5c99f3472): {'model': 'Model(\n  (lay..., bias=True)\n)', 'Model': "<class 'func6...ocals>.Model'>", 'F': "<module 'torc...unctional.py'>", 'nn': "<module 'torc...\\__init__.py'>"}
