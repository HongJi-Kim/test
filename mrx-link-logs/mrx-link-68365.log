[2022-03-14 14:59:40] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"}]}\n', cookie='_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;username-localhost-8888=2|1:0|10:1647237580|23:username-localhost-8888|44:ZDg4ZDI5MjM5ZWJhNDE5MTg5ZDllZWNkMGUxNDNhMTU=|bd44a52a204cc7eb367689cf82b8f8933864d6ac8b09162f7734f6916fa32e86', token='565c01b0408b09f041e50e66d8887d3b3ae766a50a9fcbcd')
[2022-03-14 14:59:40] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}]}
[2022-03-14 14:59:45] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"}]}\n', cookie='_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;username-localhost-8888=2|1:0|10:1647237581|23:username-localhost-8888|44:MmRlYWQ3Mzg5MDgyNDE1Yjg4MDgxM2YzZTU5ZDBiNDE=|f2cf79f8b3d9fa19e5f2a6b6e571ef1d50add4181a027261512f9e6288cddd00', token='565c01b0408b09f041e50e66d8887d3b3ae766a50a9fcbcd')
[2022-03-14 14:59:45] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}]}
[2022-03-14 15:00:22] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"\\n","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"}]}\n', cookie='_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;username-localhost-8888=2|1:0|10:1647237618|23:username-localhost-8888|44:YTgyZWU3YWE1NWQwNDlhYjg1NjgzYjgzZWU2YTdkYzI=|641bb436228d2599a04427e1d58b7fc8610989fed06f9e6e868c5c77d0b5f7e9', token='565c01b0408b09f041e50e66d8887d3b3ae766a50a9fcbcd')
[2022-03-14 15:00:22] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': '\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}]}
[2022-03-14 15:00:53] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"\\n","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"}]}\n', cookie='_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;username-localhost-8888=2|1:0|10:1647237653|23:username-localhost-8888|44:Nzc3N2ZhNzE3YjM0NGVmMThiMTk3Y2ZjZGYyNjEyZjg=|969767caa838483480e068fc4e443e228706d3ebfd46f890350209b06a31d73f', token='565c01b0408b09f041e50e66d8887d3b3ae766a50a9fcbcd')
[2022-03-14 15:00:53] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': '\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}]}
[2022-03-14 15:00:56] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"\\n","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"}]}\n', cookie='_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;username-localhost-8888=2|1:0|10:1647237656|23:username-localhost-8888|44:ZmFhNGI3MjcyYmEzNDE3NGExNGIyYzA1ODhjMTk2MzE=|a0f0143ff9e54085a3149b78da77459476c3e7015ddf994ab3ba82f105adb5e4', token='565c01b0408b09f041e50e66d8887d3b3ae766a50a9fcbcd')
[2022-03-14 15:00:56] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': '\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}]}
[2022-03-14 15:01:20] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"\\n","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"}]}\n', cookie='_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;username-localhost-8888=2|1:0|10:1647237680|23:username-localhost-8888|44:ZDk2ZTRmOWU3OTY0NGMyZDhmNDBiMWU4MjIwMDY2ZTI=|706664a69abf6fd6f95f6d5c502449fef3b3ef7d5b719f7e3dd35518ca2dd655', token='565c01b0408b09f041e50e66d8887d3b3ae766a50a9fcbcd')
[2022-03-14 15:01:20] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': '\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}]}
[2022-03-14 15:01:28] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"\\n","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"}]}\n', cookie='_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;username-localhost-8888=2|1:0|10:1647237687|23:username-localhost-8888|44:YTFjZmNjNzMyYWUyNGU3ODg5ZmFmZDRmMTg4YzFlMjQ=|80af45a5c89467d696b9af9301bd9061c28e24a52d985fd24a9e6b90e361f37d', token='565c01b0408b09f041e50e66d8887d3b3ae766a50a9fcbcd')
[2022-03-14 15:01:28] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': '\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}]}
[2022-03-14 15:01:41] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"}]}\n', cookie='_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;username-localhost-8888=2|1:0|10:1647237700|23:username-localhost-8888|44:ZmM4MGZlNjYyMDU2NDBiY2IwOTFkY2Q3OTNlZWE5Njc=|971dff32b4a09835735d5e5ff3a959898f1c4febb83a8961cf71c98bd8864662', token='565c01b0408b09f041e50e66d8887d3b3ae766a50a9fcbcd')
[2022-03-14 15:01:41] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}]}
[2022-03-14 15:01:41] mrx-link.MRXLinkDag.add_node() DEBUG: <new component (84484d20-2bd6-472d-91a1-a9c10d1bb17d)>.component_type: CodeCell -> CodeCell
[2022-03-14 15:02:11] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"}]}\n', cookie='_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;username-localhost-8888=2|1:0|10:1647237730|23:username-localhost-8888|44:MGE1NzAxNmYxYjYxNGEyMjlkZWM1MGU3MTA5ZmI5NmQ=|3ad615eba891df5964105877d34a90d2b7a26887a8563afd108e52f37d9e2959', token='565c01b0408b09f041e50e66d8887d3b3ae766a50a9fcbcd')
[2022-03-14 15:02:11] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}]}
[2022-03-14 15:02:35] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"}]}\n', cookie='_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;username-localhost-8888=2|1:0|10:1647237755|23:username-localhost-8888|44:YWVmZTIyZTVkY2ZjNDY0MWIxODdjYWVjM2YwNmIxYmE=|14c089eb7574abb201f5e0ad0479937b0a03e2b345e5e5975f775464f5dfbe91', token='565c01b0408b09f041e50e66d8887d3b3ae766a50a9fcbcd')
[2022-03-14 15:02:35] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}]}
[2022-03-14 15:02:36] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"}]}\n', cookie='_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;username-localhost-8888=2|1:0|10:1647237756|23:username-localhost-8888|44:NGQwZGE3M2ZhNmJkNGIyZmEwZTQ2YWZjZWQwOGMzNjg=|2b40e66c3cc65be457fec1b9c11b4d8464deeab137acd93356cb6a56f8e0d86f', token='565c01b0408b09f041e50e66d8887d3b3ae766a50a9fcbcd')
[2022-03-14 15:02:36] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}]}
[2022-03-14 15:02:53] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"}]}\n', cookie='_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;username-localhost-8888=2|1:0|10:1647237769|23:username-localhost-8888|44:MmUyNmE2MDJkMjUyNGUzNTkyZWZiOTg3NDM0MjExOWI=|25ccba3e58fa9aeff5b64393bfa4e6703a9fe7de24b06c998fdd7cc4e5f4bd75', token='565c01b0408b09f041e50e66d8887d3b3ae766a50a9fcbcd')
[2022-03-14 15:02:53] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}]}
[2022-03-14 15:03:10] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie='_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;username-localhost-8888=2|1:0|10:1647237789|23:username-localhost-8888|44:YzRmOWU3NjE5MDcwNGNiNGFmNDAzZTY4Y2UwMGY1ODk=|e94b7c65459e50caa211590a82af1a7d0472fce6f9bab06bff6aef39fe8404f0', token='565c01b0408b09f041e50e66d8887d3b3ae766a50a9fcbcd')
[2022-03-14 15:03:10] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-14 15:03:12] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie='_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;username-localhost-8888=2|1:0|10:1647237790|23:username-localhost-8888|44:OTg5NjNkMzBiYzliNDcwZGI3NDk5N2M2ZWIxYzNmMDY=|bd788ad9a04b7f0be18229f7a7d048563e9f693d6db1627fddb7b72dfbde71e0', token='565c01b0408b09f041e50e66d8887d3b3ae766a50a9fcbcd')
[2022-03-14 15:03:12] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-14 15:03:15] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie='_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;username-localhost-8888=2|1:0|10:1647237792|23:username-localhost-8888|44:MzY3Yzk2ZTYwYzFlNDMxZmExY2QzNzZjZDQzNDRmZjk=|6dbae879e871e45f72bb5f415118bc216ed751a3c8d78a601bc0541fca58ed0c', token='565c01b0408b09f041e50e66d8887d3b3ae766a50a9fcbcd')
[2022-03-14 15:03:15] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-14 15:03:24] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie='_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;username-localhost-8888=2|1:0|10:1647237804|23:username-localhost-8888|44:YTRhNWEwZTM3NzllNDA3ZGJmNTRlMTZhNDM5YTc3M2U=|5e045c8ebbfd12a2ef3eb4a2c2c59f5e2dd69c789144bf2e0d74b0042d40f46b', token='565c01b0408b09f041e50e66d8887d3b3ae766a50a9fcbcd')
[2022-03-14 15:03:24] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-14 15:03:37] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie='_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;username-localhost-8888=2|1:0|10:1647237817|23:username-localhost-8888|44:YmE0YTc2NDE4ODY4NGVhNjkxNTM2MTcxOTkzMGY0ZjY=|68a216cad8cecc3e1f66bf18d54ce19b0139ceca321dedff713a2ef28db7cff4', token='565c01b0408b09f041e50e66d8887d3b3ae766a50a9fcbcd')
[2022-03-14 15:03:37] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-14 15:03:58] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie='_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;username-localhost-8888=2|1:0|10:1647237837|23:username-localhost-8888|44:NGFhYzlhNzNhYmE0NDM0MzliNGZkZDkwNDBlMmE2NGE=|f9857411f5e7326d2551fc6bfc44de2c4ea2ff788343ef215d31f191f9f1c43a', token='565c01b0408b09f041e50e66d8887d3b3ae766a50a9fcbcd')
[2022-03-14 15:03:58] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-14 15:04:00] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie='_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;username-localhost-8888=2|1:0|10:1647237839|23:username-localhost-8888|44:Njc0MDJlMTljYjcwNDRlNDg3MmU4ZGYwMmIwNDFkYjU=|aae366d665825c9606ad5bdb84b3afad5bd4c28cc8e0b506e3be7bc2d8cd049c', token='565c01b0408b09f041e50e66d8887d3b3ae766a50a9fcbcd')
[2022-03-14 15:04:00] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-14 15:16:28] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie='_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;username-localhost-8888=2|1:0|10:1647238588|23:username-localhost-8888|44:NmVkOWJjMmMzMzExNGQ2N2FmNTA2ODg2YjdiYTdmZDk=|5efca0a9eeb70b5b7a52f40b0a6a18435e27bd115a32a97344f23bb58d1816b6', token='565c01b0408b09f041e50e66d8887d3b3ae766a50a9fcbcd')
[2022-03-14 15:16:28] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-15 13:15:05] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie='_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;username-localhost-8888=2|1:0|10:1647317705|23:username-localhost-8888|44:NGE2ZWUxNWM0ZjNiNDY4M2I1NzljN2I2NjA4Njg5N2E=|c39fba98d4b0f0f3c38b0aee99f8e7fc6ca7078487d1944fce8a699f15ea308a', token='565c01b0408b09f041e50e66d8887d3b3ae766a50a9fcbcd')
[2022-03-15 13:15:05] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-15 15:40:55] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie='_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;username-localhost-8888=2|1:0|10:1647326455|23:username-localhost-8888|44:ODQ1MWFmMGNjM2FhNDE0MmFhZGE4MTNmNTAwNTQzNTA=|061567c2f3c7bfa8526031d9a3b319bf5016f6e6f2de8eae8d2ef28d67263f78', token='565c01b0408b09f041e50e66d8887d3b3ae766a50a9fcbcd')
[2022-03-15 15:40:55] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
