[2022-03-24 15:59:37] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie='_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;username-localhost-8888=2|1:0|10:1648105177|23:username-localhost-8888|44:ZDc3NDZhM2Q0NjUxNDgzNGExY2FjNWQ3YWUzNjQ1ODE=|9f6788192e6ecfa26cf45d8e94fbcb7924a6dc161aa4a6d3049f9c89557ba80d', token='cbf3f61e28881b8ded447af46f94ce9d43b53bb321b3f8a8')
[2022-03-24 15:59:37] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-24 15:59:50] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie='_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;username-localhost-8888=2|1:0|10:1648105190|23:username-localhost-8888|44:ZTk5OGU0ZTkzNzRmNGI0NDg3MWFlMDk5YjVlMWI3ZWY=|6f0b9a7d7f1264a0092ae627381b5669df4f418bb1cfa142dfc2ef2068ad52da', token='cbf3f61e28881b8ded447af46f94ce9d43b53bb321b3f8a8')
[2022-03-24 15:59:50] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-24 16:02:11] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie='_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;username-localhost-8888=2|1:0|10:1648105331|23:username-localhost-8888|44:NzJiM2I0ZWY5NWFlNDViYmIzZjExYjlkM2NmMzg2N2U=|f90a10546ece7936826a1ca339f98ac47b17ab9c4678328a83bef663da0f93ee', token='cbf3f61e28881b8ded447af46f94ce9d43b53bb321b3f8a8')
[2022-03-24 16:02:11] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-24 16:05:13] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie='_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;username-localhost-8888=2|1:0|10:1648105512|23:username-localhost-8888|44:MzM1Y2FmMTczMDMzNDlkNmE4NTRhYzVkNTdhZDJmZTI=|b6a59072a436a3ff3bcfad66a63edb6848d9ae863a8e1be44c9e666f12e32043', token='cbf3f61e28881b8ded447af46f94ce9d43b53bb321b3f8a8')
[2022-03-24 16:05:13] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-24 16:21:50] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie='_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;username-localhost-8888=2|1:0|10:1648106510|23:username-localhost-8888|44:OWE4YTA5ZWEwMmYxNDUxMjk1NGViNzU3NWEyOTFlMzc=|4a9ea869820b8b21065065e09415fee5d2f8d1dff1adbd7f637723b6c5bb8b43', token='cbf3f61e28881b8ded447af46f94ce9d43b53bb321b3f8a8')
[2022-03-24 16:21:50] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-24 16:23:14] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie='_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;_xsrf=2|013c2e93|44472d2f93d7de48730a6a3d753c7a6f|1645781336;username-localhost-8888=2|1:0|10:1648106594|23:username-localhost-8888|44:OTJhYmQ4NWU5MmY4NDM5M2IyOWM1NDhjMDJkYjYzYTQ=|e8010adb4b8a87a043106b8a410bd651c0a21149eb0939fbdb0b80f1468d6857', token='cbf3f61e28881b8ded447af46f94ce9d43b53bb321b3f8a8')
[2022-03-24 16:23:14] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-28 10:09:32] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie=';username-localhost-8888=2|1:0|10:1648429771|23:username-localhost-8888|44:NmRmMzQ3NDUyZjZiNDI5M2E5ZjY0YTE0N2YxZWJhZjg=|3c3a6b1434af1d83a9b0a20ef52dd0ac0ce07bf8b1c7cc770b1887ca85364859', token='cbf3f61e28881b8ded447af46f94ce9d43b53bb321b3f8a8')
[2022-03-28 10:09:32] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-28 10:09:35] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie=';username-localhost-8888=2|1:0|10:1648429772|23:username-localhost-8888|44:MmNiNmEwMzhlNDljNDFkMTgyMDAwNTMwYjlkMGFlYmY=|219911accabaa7209bd7713005d3a5a81ce1ba01f2edf3cc4089e185b9646682', token='cbf3f61e28881b8ded447af46f94ce9d43b53bb321b3f8a8')
[2022-03-28 10:09:35] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-28 10:42:37] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie=';username-localhost-8888=2|1:0|10:1648431756|23:username-localhost-8888|44:NmMzNzI3OGVmYjY3NGE4Yjg1YzdkY2RiODQ2ODM2MTY=|702866e9c4b0677caaa88db8e71a47459102aa8f2ec2c92ad206cc8d7e5295dd', token='cbf3f61e28881b8ded447af46f94ce9d43b53bb321b3f8a8')
[2022-03-28 10:42:37] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-28 16:05:16] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie=';username-localhost-8888=2|1:0|10:1648451115|23:username-localhost-8888|44:Yzg1OGJhOGI1NTRmNGE0NWI0NzU2YTY0NTg5MTVlNmQ=|9de7489da152d85618b12dff846d9b9eb1a6717670adcfd94ad49ae52737bfbd', token='cbf3f61e28881b8ded447af46f94ce9d43b53bb321b3f8a8')
[2022-03-28 16:05:16] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-28 16:12:34] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie=';username-localhost-8888=2|1:0|10:1648451553|23:username-localhost-8888|44:Y2NiZDY3MWU0MGJkNGVkMzg5ZjRlYWNmNzRkZTU1ZjY=|2c8d61ed8ed073a77abd351e2ce7c484225954383a8672c0d9ac81fbfc283e77', token='cbf3f61e28881b8ded447af46f94ce9d43b53bb321b3f8a8')
[2022-03-28 16:12:34] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-28 16:16:26] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie=';username-localhost-8888=2|1:0|10:1648451780|23:username-localhost-8888|44:MGVkYmUyYzgxM2RhNDM3Y2IzZDk5MTNjOTE2MDA0MDM=|b4729d6c79ab4c0026b1936a669f8be12b4bbbc9ea00146e5c5a1e47442ccdec', token='cbf3f61e28881b8ded447af46f94ce9d43b53bb321b3f8a8')
[2022-03-28 16:16:26] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-28 16:16:28] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie=';username-localhost-8888=2|1:0|10:1648451786|23:username-localhost-8888|44:YjE1YzdlMzZlYWRhNGZiZWEwMjlkNDE3YjlkNmJhODQ=|291fcacd6d404b8f6ea9d6df51af82eea0770f5551de0bb79fdf21dd7ed56b97', token='cbf3f61e28881b8ded447af46f94ce9d43b53bb321b3f8a8')
[2022-03-28 16:16:28] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-28 16:16:29] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie=';username-localhost-8888=2|1:0|10:1648451788|23:username-localhost-8888|44:MzdlNzhhYjI0M2RlNDM1Zjg1ZTZiYjY0NmVjMzBkYTk=|faa33d7dea83c8dacb7e2ceceb7a1bd8cb32aef8dd592e6af051dfd366c2a949', token='cbf3f61e28881b8ded447af46f94ce9d43b53bb321b3f8a8')
[2022-03-28 16:16:29] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-28 16:16:30] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie=';username-localhost-8888=2|1:0|10:1648451789|23:username-localhost-8888|44:ZmYyNWZkNmNkZDY0NDMzZDhhNjBlNzgxNTMyNGZkMmE=|49c793c70a71657332f6a9fb30a1aceba0a69a97014f7a2331230ef8b63c8eeb', token='cbf3f61e28881b8ded447af46f94ce9d43b53bb321b3f8a8')
[2022-03-28 16:16:30] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-28 16:16:31] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie=';username-localhost-8888=2|1:0|10:1648451790|23:username-localhost-8888|44:YzAyM2Q0M2I5ZjkzNDg3NmE0OTU5Zjg3MzRlYjQ1Mjg=|7e10ad7eb8c032713f9a258e236cab2866e28d7229c3f6e9a7f8c03a83391ffe', token='cbf3f61e28881b8ded447af46f94ce9d43b53bb321b3f8a8')
[2022-03-28 16:16:31] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-28 16:16:32] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie=';username-localhost-8888=2|1:0|10:1648451791|23:username-localhost-8888|44:Yjc1MDg5YzI3ZDZlNDczNTliNGNlNDUxMjdlNjFiYWY=|c179c7e4835d0aa5cdee4a6c8019045c71429ff782cff7e9392b368158a838fd', token='cbf3f61e28881b8ded447af46f94ce9d43b53bb321b3f8a8')
[2022-03-28 16:16:32] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-28 16:16:34] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie=';username-localhost-8888=2|1:0|10:1648451793|23:username-localhost-8888|44:NTY3YjUzNWE2NzdhNGJkMmE4MWEyMmY0ZjU4ODRlNjA=|9ba7673a39f3dbe160f768f46961fd12fc9b1719d790b123aacfcad626efe1fe', token='cbf3f61e28881b8ded447af46f94ce9d43b53bb321b3f8a8')
[2022-03-28 16:16:34] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-28 16:16:35] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie=';username-localhost-8888=2|1:0|10:1648451794|23:username-localhost-8888|44:OWIwNWQ4ODA0MjExNDg5ZDljN2VhODFmZTU5MTY1NzQ=|9f499b92183d0a11a313c63534db28f6af2e5bc3d6166cb8ea51f034b872242d', token='cbf3f61e28881b8ded447af46f94ce9d43b53bb321b3f8a8')
[2022-03-28 16:16:35] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-28 16:16:43] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie=';username-localhost-8888=2|1:0|10:1648451800|23:username-localhost-8888|44:ODE2MTc1NDQ5MThmNDJmZjg1MDNlNzlkNGFjZWEzMGE=|35f1c1e9b90f4515c110bd5c69d1d2c3be3f4ca6bcf62913c7d35c302aceabed', token='cbf3f61e28881b8ded447af46f94ce9d43b53bb321b3f8a8')
[2022-03-28 16:16:43] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-28 16:27:10] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie=';username-localhost-8888=2|1:0|10:1648452430|23:username-localhost-8888|44:ZDJkNDI1ZDZkMDkzNDlkMWI4ZWFmN2RmZjhmMzNkMGY=|aa56a19f54a9403ed2482934008156d5023e47e243fd0bd69fd51c25ff2bcf51', token='cbf3f61e28881b8ded447af46f94ce9d43b53bb321b3f8a8')
[2022-03-28 16:27:10] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-28 16:27:58] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie=';username-localhost-8888=2|1:0|10:1648452478|23:username-localhost-8888|44:MjM5NmVlYmRlY2QwNGQ3OWI3OWNmYjU3YzM0MzNlMDU=|63742916c167e617b2d64e17d12bf8798ed36fe8ab1f99cb848c1999d1179e34', token='cbf3f61e28881b8ded447af46f94ce9d43b53bb321b3f8a8')
[2022-03-28 16:27:58] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
[2022-03-28 16:29:17] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://localhost:8888/', cell='{"nodes":[{"id":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","name":"Read data","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"#00DE62","comments":[]}},{"id":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","name":"Data loader","code":"def __pdc_data_loader__(  # type: ignore\\n    input_df: str,\\n    ratio: float,\\n    batch_size: int,\\n    batch_shuffle: bool,\\n    tv_shuffle: bool,\\n    drop_last: bool,\\n    target: list,  # type: ignore\\n):\\n    \\"\\"\\"read data from database\\"\\"\\"\\n    import random\\n    from typing import Any, Optional, Tuple\\n\\n    import pandas as pd\\n    import torch\\n    from torch.utils.data import DataLoader, Dataset\\n\\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\\n            super().__init__()\\n            self.data = torch.from_numpy(data.values)\\n            if target is not None:\\n                self.target = torch.from_numpy(target.values)\\n                if target.shape[-1] == 1:\\n                    self.target = torch.from_numpy(target.values.squeeze())\\n                else:\\n                    self.target = torch.from_numpy(target.values)\\n            else:\\n                self.target = target\\n\\n        def __len__(self) -> int:\\n            return len(self.data)\\n\\n        def __getitem__(self, idx: int) -> Any:\\n            if self.target is not None:\\n                return self.data[idx], self.target[idx]\\n            return self.data[idx]\\n\\n    # pylint: disable=missing-function-docstring\\n    def split(\\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\\n\\n        data_cnt = int(data.shape[0] * ratio)\\n\\n        indices = list(data.index)\\n        if shuffle:\\n            random.shuffle(indices)\\n\\n        if target is not None:\\n            return (\\n                (\\n                    pd.DataFrame(data, index=indices[:data_cnt]),\\n                    pd.DataFrame(target, index=indices[:data_cnt]),\\n                ),\\n                (\\n                    pd.DataFrame(data, index=indices[data_cnt:]),\\n                    pd.DataFrame(target, index=indices[data_cnt:]),\\n                ),\\n            )\\n        return (\\n            (\\n                pd.DataFrame(data, index=indices[:data_cnt]),\\n                None,\\n            ),\\n            (\\n                pd.DataFrame(data, index=indices[data_cnt:]),\\n                None,\\n            ),\\n        )\\n\\n    data: pd.DataFrame = input_df\\n    train_loader, valid_loader = None, None\\n    if target:\\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\\n        target_df = pd.DataFrame(data, columns=target)\\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0], target=train[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n    else:\\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\\n        if train[0].shape[0] > 0:\\n            train_loader = DataLoader(\\n                dataset=SampleDataset(data=train[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n        if valid[0].shape[0] > 0:\\n            valid_loader = DataLoader(\\n                dataset=SampleDataset(data=valid[0]),\\n                batch_size=batch_size,\\n                shuffle=batch_shuffle,\\n                drop_last=drop_last,\\n            )\\n\\n    return train_loader, valid_loader\\n\\ntrain, valid = __pdc_data_loader__(\\n    input_df=df,\\n    ratio=0.8,\\n    batch_size=4,\\n    batch_shuffle=True,\\n    tv_shuffle=True,\\n    drop_last=True,\\n    target=[\'class\'],\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"62535059-9784-42d0-9799-29d5c99f3472","name":"Modeling","code":"from torch import nn\\nfrom torch.nn import functional as F  \\n\\nclass Model(nn.Module):\\n    def __init__(self, input_dim):\\n        super(Model, self).__init__()\\n        self.layer1 = nn.Linear(input_dim, 50)\\n        self.layer2 = nn.Linear(50, 50)\\n        self.layer3 = nn.Linear(50, 3)\\n    def forward(self, x):\\n        x = F.relu(self.layer1(x))\\n        x = F.relu(self.layer2(x))\\n        x = F.softmax(self.layer3(x), dim=1)\\n        return x\\n    \\nmodel = Model(4).double()","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","name":"Training","code":"def __pdc_trainer__(  # type: ignore # noqa: C901\\n    is_classification: int,\\n    train_loader,  # DataLoader\\n    valid_loader,  # DataLoader\\n    train_metric_members,  # List[str]\\n    valid_metric_members,  # List[str]\\n    monitoring: str,\\n    early_stopping_mode: str,\\n    model,  # nn.Module\\n    loss_impl,  # nn.Module\\n    optimizer_impl,  # nn.optim\\n    learning_rate: float,\\n    n_epochs: int,\\n):\\n    from collections import defaultdict\\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\\n\\n    import pytorch_lightning as pl\\n    import torch\\n    import torchmetrics\\n    from livelossplot import PlotLosses\\n    from pytorch_lightning.callbacks import EarlyStopping\\n    from pytorch_lightning.loggers import LightningLoggerBase\\n    from pytorch_lightning.utilities.distributed import rank_zero_only\\n    from torch import nn\\n    from torch.optim import Optimizer\\n\\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\\n        \\"\\"\\"Canvas Trainer Logger\\"\\"\\"\\n\\n        def __init__(self) -> None:\\n            super().__init__()\\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\\n            self.live_metric = PlotLosses()\\n\\n        @property\\n        def experiment(self) -> None:\\n            pass\\n\\n        @property\\n        def name(self) -> str:\\n            return \\"CanvasTrainerLogger\\"\\n\\n        @property\\n        def version(self) -> str:\\n            return \\"mvp\\"\\n\\n        @rank_zero_only\\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\\n            pass\\n\\n        @rank_zero_only\\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\\n            del metrics[\\"epoch\\"]\\n\\n            if not metrics.keys():\\n                return\\n\\n            is_valid = list(metrics.keys())[-1][:4] == \\"val_\\"\\n\\n            if is_valid:\\n                self.metric_history[\\"val\\"].append(metrics)\\n            else:\\n                self.metric_history[\\"train\\"].append(metrics)\\n                self.live_metric.update({**self.metric_history[\\"train\\"][-1], **self.metric_history[\\"val\\"][-1]})\\n                self.live_metric.send()\\n\\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\\n        def __init__(  # pylint: disable=too-many-arguments\\n            self,\\n            model: nn.Module,\\n            loss_impl: nn.Module,\\n            optimizer_impl: Optimizer,\\n            learning_rate: float,\\n            metric_members: Dict[str, Any],\\n            is_classification: int,\\n        ) -> None:\\n            super().__init__()\\n            self.model = model\\n\\n            self.loss_func = loss_impl()\\n            self.optimizer_impl = optimizer_impl\\n            self.learning_rate = learning_rate\\n\\n            self.metric_members = metric_members\\n\\n            for mode in [\\"train\\", \\"val\\"]:\\n                auroc_metric_attr = (mode + \\"_\\" if mode != \\"train\\" else \\"\\") + \\"auroc\\"\\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get(\\"AUROC\\"))\\n\\n            self._is_classification = is_classification\\n\\n        # pylint: disable=invalid-name\\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\\n            \\"\\"\\"calculate log metric with metric members\\"\\"\\"\\n            metric_members, prefix = None, \\"\\"\\n            if is_valid:\\n                metric_members = self.metric_members[\\"val\\"]\\n                prefix += \\"val_\\"\\n            else:\\n                metric_members = self.metric_members[\\"train\\"]\\n\\n            for metric_member_str, metric_impl in metric_members.items():\\n                if metric_member_str == \\"AUROC\\":\\n                    getattr(self, prefix + \\"auroc\\")(y_hat.detach(), y.detach())\\n                    try:\\n                        self.log(\\n                            prefix + metric_member_str, getattr(self, prefix + \\"auroc\\"), on_step=False, on_epoch=True\\n                        )\\n                    except ValueError():  # type: ignore\\n                        pass\\n\\n                else:\\n                    metric = metric_impl(y_hat, y)\\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\\n\\n        # pylint: disable=arguments-differ\\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=False)\\n\\n        # pylint: disable=arguments-differ\\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\\n            return self._step_func(batch, is_valid=True)\\n\\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\\n            x, y = batch  # pylint: disable=invalid-name\\n            if self._is_classification:\\n                y = y.long()  # pylint: disable=invalid-name\\n            y_hat = self.model(x)\\n\\n            loss = self.loss_func(y_hat, y)\\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\\n            return loss\\n\\n        def configure_optimizers(self) -> Optimizer:\\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\\n\\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\\n        \\"\\"\\"load metric implementation on torch.nn or torchmetrics with their names\\"\\"\\"\\n        metric_impls = {}\\n        for member_str in members:\\n            if member_str == \\"AUROC\\":\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\\n                    num_classes=num_classes, compute_on_step=False\\n                )\\n\\n            elif member_str in [\\"CrossEntropyLoss\\", \\"MSELoss\\", \\"L1Loss\\"]:\\n                metric_impls[member_str] = getattr(nn, member_str)()\\n\\n            elif member_str in [\\"Accuracy\\", \\"AUROC\\", \\"R2Score\\"]:\\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\\n        return metric_impls\\n\\n    # trainer main\\n    n_classes = None\\n    if is_classification:\\n        labels = torch.cat([y for _, y in valid_loader])\\n        if len(labels.unique()) > 2:\\n            n_classes = len(labels.unique())\\n\\n    metric_members = dict(\\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\\n    )\\n\\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\\n    early_stopping_callback = EarlyStopping(monitor=\\"val_\\" + monitoring, mode=early_stopping_mode)\\n    lit = LitModule(\\n        model=model,\\n        loss_impl=loss_impl,\\n        optimizer_impl=optimizer_impl,\\n        learning_rate=learning_rate,\\n        metric_members=metric_members,\\n        is_classification=is_classification,\\n    )\\n    trainer = pl.Trainer(\\n        max_epochs=n_epochs,\\n        logger=canvas_trainer_logger,\\n        callbacks=[early_stopping_callback],\\n        checkpoint_callback=False,\\n    )\\n    trainer.fit(lit, train_loader, valid_loader)\\n    return (canvas_trainer_logger.metric_history, model, model)\\n\\nimport torch\\nc, b, a = __pdc_trainer__(\\n    is_classification=1,\\n    train_loader=train,\\n    valid_loader=valid,\\n    train_metric_members=[\'CrossEntropyLoss\'],\\n    valid_metric_members=[\'CrossEntropyLoss\'],\\n    monitoring=\'CrossEntropyLoss\',\\n    early_stopping_mode=\'min\',\\n    model=model,\\n    loss_impl=torch.nn.CrossEntropyLoss,\\n    optimizer_impl=torch.optim.Adam,\\n    learning_rate=learning_rate,\\n    n_epochs=n_epochs,\\n)\\n","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}},{"id":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","name":"new component","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[]}},{"id":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07","name":"new component_2","code":"import pandas as pd\\ndf = pd.DataFrame([1,2,3])\\ndf","metadata":{"componentType":"CodeCell","headerColor":"none","comments":[]}}],"edges":[{"parent":"ab2d95a2-220f-4f7d-8c1a-ef03b514587e","child":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642"},{"parent":"62535059-9784-42d0-9799-29d5c99f3472","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"ecb7141a-ce5c-4fc3-be3b-f42abe7d3642","child":"1a49b14c-abaf-4f75-b146-e43813e2e4fb"},{"parent":"1a49b14c-abaf-4f75-b146-e43813e2e4fb","child":"84484d20-2bd6-472d-91a1-a9c10d1bb17d"},{"parent":"84484d20-2bd6-472d-91a1-a9c10d1bb17d","child":"9dbcb0cb-255c-4aca-95d0-4aaa1e199b07"}]}\n', cookie=';username-localhost-8888=2|1:0|10:1648452557|23:username-localhost-8888|44:NzY4MmJhYmUyNmI4NDgxOGJlY2FjNDc3YmM4MTdiM2E=|198bf359b6b997408705293d81f77fa59e4a2d96ef24935a49fd0a1b1ec2a077', token='cbf3f61e28881b8ded447af46f94ce9d43b53bb321b3f8a8')
[2022-03-28 16:29:17] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'name': 'Read data', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': '#00DE62', 'comments': []}}, {'id': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'name': 'Data loader', 'code': 'def __pdc_data_loader__(  # type: ignore\n    input_df: str,\n    ratio: float,\n    batch_size: int,\n    batch_shuffle: bool,\n    tv_shuffle: bool,\n    drop_last: bool,\n    target: list,  # type: ignore\n):\n    """read data from database"""\n    import random\n    from typing import Any, Optional, Tuple\n\n    import pandas as pd\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n\n    class SampleDataset(Dataset):  # pylint: disable=missing-class-docstring\n        def __init__(self, data: pd.DataFrame, target: Optional[pd.DataFrame] = None) -> None:\n            super().__init__()\n            self.data = torch.from_numpy(data.values)\n            if target is not None:\n                self.target = torch.from_numpy(target.values)\n                if target.shape[-1] == 1:\n                    self.target = torch.from_numpy(target.values.squeeze())\n                else:\n                    self.target = torch.from_numpy(target.values)\n            else:\n                self.target = target\n\n        def __len__(self) -> int:\n            return len(self.data)\n\n        def __getitem__(self, idx: int) -> Any:\n            if self.target is not None:\n                return self.data[idx], self.target[idx]\n            return self.data[idx]\n\n    # pylint: disable=missing-function-docstring\n    def split(\n        data: pd.DataFrame, ratio: float, target: Optional[pd.DataFrame] = None, shuffle: bool = False\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\n        data_cnt = int(data.shape[0] * ratio)\n\n        indices = list(data.index)\n        if shuffle:\n            random.shuffle(indices)\n\n        if target is not None:\n            return (\n                (\n                    pd.DataFrame(data, index=indices[:data_cnt]),\n                    pd.DataFrame(target, index=indices[:data_cnt]),\n                ),\n                (\n                    pd.DataFrame(data, index=indices[data_cnt:]),\n                    pd.DataFrame(target, index=indices[data_cnt:]),\n                ),\n            )\n        return (\n            (\n                pd.DataFrame(data, index=indices[:data_cnt]),\n                None,\n            ),\n            (\n                pd.DataFrame(data, index=indices[data_cnt:]),\n                None,\n            ),\n        )\n\n    data: pd.DataFrame = input_df\n    train_loader, valid_loader = None, None\n    if target:\n        data_df = pd.DataFrame(data, columns=[x for x in data.columns if x not in target])\n        target_df = pd.DataFrame(data, columns=target)\n        train, valid = split(data_df, ratio, target_df, tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0], target=train[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0], target=valid[1]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n    else:\n        train, valid = split(data, ratio, shuffle=tv_shuffle)\n        if train[0].shape[0] > 0:\n            train_loader = DataLoader(\n                dataset=SampleDataset(data=train[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n        if valid[0].shape[0] > 0:\n            valid_loader = DataLoader(\n                dataset=SampleDataset(data=valid[0]),\n                batch_size=batch_size,\n                shuffle=batch_shuffle,\n                drop_last=drop_last,\n            )\n\n    return train_loader, valid_loader\n\ntrain, valid = __pdc_data_loader__(\n    input_df=df,\n    ratio=0.8,\n    batch_size=4,\n    batch_shuffle=True,\n    tv_shuffle=True,\n    drop_last=True,\n    target=[\'class\'],\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '62535059-9784-42d0-9799-29d5c99f3472', 'name': 'Modeling', 'code': 'from torch import nn\nfrom torch.nn import functional as F  \n\nclass Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 50)\n        self.layer2 = nn.Linear(50, 50)\n        self.layer3 = nn.Linear(50, 3)\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.softmax(self.layer3(x), dim=1)\n        return x\n    \nmodel = Model(4).double()', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'name': 'Training', 'code': 'def __pdc_trainer__(  # type: ignore # noqa: C901\n    is_classification: int,\n    train_loader,  # DataLoader\n    valid_loader,  # DataLoader\n    train_metric_members,  # List[str]\n    valid_metric_members,  # List[str]\n    monitoring: str,\n    early_stopping_mode: str,\n    model,  # nn.Module\n    loss_impl,  # nn.Module\n    optimizer_impl,  # nn.optim\n    learning_rate: float,\n    n_epochs: int,\n):\n    from collections import defaultdict\n    from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n    import pytorch_lightning as pl\n    import torch\n    import torchmetrics\n    from livelossplot import PlotLosses\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_lightning.loggers import LightningLoggerBase\n    from pytorch_lightning.utilities.distributed import rank_zero_only\n    from torch import nn\n    from torch.optim import Optimizer\n\n    class CanvasTrainerLightningLogger(LightningLoggerBase):\n        """Canvas Trainer Logger"""\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.metric_history: DefaultDict[str, List[Any]] = defaultdict(list)\n            self.live_metric = PlotLosses()\n\n        @property\n        def experiment(self) -> None:\n            pass\n\n        @property\n        def name(self) -> str:\n            return "CanvasTrainerLogger"\n\n        @property\n        def version(self) -> str:\n            return "mvp"\n\n        @rank_zero_only\n        def log_hyperparams(self, params: Any) -> None:  # pylint: disable=arguments-differ\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n            del metrics["epoch"]\n\n            if not metrics.keys():\n                return\n\n            is_valid = list(metrics.keys())[-1][:4] == "val_"\n\n            if is_valid:\n                self.metric_history["val"].append(metrics)\n            else:\n                self.metric_history["train"].append(metrics)\n                self.live_metric.update({**self.metric_history["train"][-1], **self.metric_history["val"][-1]})\n                self.live_metric.send()\n\n    class LitModule(pl.LightningModule):  # pylint: disable=too-many-ancestors, missing-class-docstring\n        def __init__(  # pylint: disable=too-many-arguments\n            self,\n            model: nn.Module,\n            loss_impl: nn.Module,\n            optimizer_impl: Optimizer,\n            learning_rate: float,\n            metric_members: Dict[str, Any],\n            is_classification: int,\n        ) -> None:\n            super().__init__()\n            self.model = model\n\n            self.loss_func = loss_impl()\n            self.optimizer_impl = optimizer_impl\n            self.learning_rate = learning_rate\n\n            self.metric_members = metric_members\n\n            for mode in ["train", "val"]:\n                auroc_metric_attr = (mode + "_" if mode != "train" else "") + "auroc"\n                setattr(self, auroc_metric_attr, self.metric_members[mode].get("AUROC"))\n\n            self._is_classification = is_classification\n\n        # pylint: disable=invalid-name\n        def _calculate_log_metric(self, y_hat: torch.Tensor, y: torch.Tensor, is_valid: bool = False) -> None:\n            """calculate log metric with metric members"""\n            metric_members, prefix = None, ""\n            if is_valid:\n                metric_members = self.metric_members["val"]\n                prefix += "val_"\n            else:\n                metric_members = self.metric_members["train"]\n\n            for metric_member_str, metric_impl in metric_members.items():\n                if metric_member_str == "AUROC":\n                    getattr(self, prefix + "auroc")(y_hat.detach(), y.detach())\n                    try:\n                        self.log(\n                            prefix + metric_member_str, getattr(self, prefix + "auroc"), on_step=False, on_epoch=True\n                        )\n                    except ValueError():  # type: ignore\n                        pass\n\n                else:\n                    metric = metric_impl(y_hat, y)\n                    self.log(prefix + metric_member_str, metric, on_step=False, on_epoch=True)\n\n        # pylint: disable=arguments-differ\n        def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=False)\n\n        # pylint: disable=arguments-differ\n        def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _: Any) -> torch.Tensor:\n            return self._step_func(batch, is_valid=True)\n\n        def _step_func(self, batch: Tuple[torch.Tensor, torch.Tensor], is_valid: bool = False) -> torch.Tensor:\n            x, y = batch  # pylint: disable=invalid-name\n            if self._is_classification:\n                y = y.long()  # pylint: disable=invalid-name\n            y_hat = self.model(x)\n\n            loss = self.loss_func(y_hat, y)\n            self._calculate_log_metric(y_hat, y, is_valid=is_valid)\n            return loss\n\n        def configure_optimizers(self) -> Optimizer:\n            return self.optimizer_impl(self.parameters(), lr=self.learning_rate)\n\n    def load_metric_impl(members: List[str], num_classes: Optional[int] = None) -> Optimizer:\n        """load metric implementation on torch.nn or torchmetrics with their names"""\n        metric_impls = {}\n        for member_str in members:\n            if member_str == "AUROC":\n                metric_impls[member_str] = getattr(torchmetrics, member_str)(\n                    num_classes=num_classes, compute_on_step=False\n                )\n\n            elif member_str in ["CrossEntropyLoss", "MSELoss", "L1Loss"]:\n                metric_impls[member_str] = getattr(nn, member_str)()\n\n            elif member_str in ["Accuracy", "AUROC", "R2Score"]:\n                metric_impls[member_str] = getattr(torchmetrics, member_str)()\n        return metric_impls\n\n    # trainer main\n    n_classes = None\n    if is_classification:\n        labels = torch.cat([y for _, y in valid_loader])\n        if len(labels.unique()) > 2:\n            n_classes = len(labels.unique())\n\n    metric_members = dict(\n        train=load_metric_impl(train_metric_members, num_classes=n_classes),\n        val=load_metric_impl(valid_metric_members, num_classes=n_classes),\n    )\n\n    canvas_trainer_logger = CanvasTrainerLightningLogger()\n    early_stopping_callback = EarlyStopping(monitor="val_" + monitoring, mode=early_stopping_mode)\n    lit = LitModule(\n        model=model,\n        loss_impl=loss_impl,\n        optimizer_impl=optimizer_impl,\n        learning_rate=learning_rate,\n        metric_members=metric_members,\n        is_classification=is_classification,\n    )\n    trainer = pl.Trainer(\n        max_epochs=n_epochs,\n        logger=canvas_trainer_logger,\n        callbacks=[early_stopping_callback],\n        checkpoint_callback=False,\n    )\n    trainer.fit(lit, train_loader, valid_loader)\n    return (canvas_trainer_logger.metric_history, model, model)\n\nimport torch\nc, b, a = __pdc_trainer__(\n    is_classification=1,\n    train_loader=train,\n    valid_loader=valid,\n    train_metric_members=[\'CrossEntropyLoss\'],\n    valid_metric_members=[\'CrossEntropyLoss\'],\n    monitoring=\'CrossEntropyLoss\',\n    early_stopping_mode=\'min\',\n    model=model,\n    loss_impl=torch.nn.CrossEntropyLoss,\n    optimizer_impl=torch.optim.Adam,\n    learning_rate=learning_rate,\n    n_epochs=n_epochs,\n)\n', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}, {'id': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'name': 'new component', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': []}}, {'id': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07', 'name': 'new component_2', 'code': 'import pandas as pd\ndf = pd.DataFrame([1,2,3])\ndf', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'none', 'comments': []}}], 'edges': [{'parent': 'ab2d95a2-220f-4f7d-8c1a-ef03b514587e', 'child': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642'}, {'parent': '62535059-9784-42d0-9799-29d5c99f3472', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': 'ecb7141a-ce5c-4fc3-be3b-f42abe7d3642', 'child': '1a49b14c-abaf-4f75-b146-e43813e2e4fb'}, {'parent': '1a49b14c-abaf-4f75-b146-e43813e2e4fb', 'child': '84484d20-2bd6-472d-91a1-a9c10d1bb17d'}, {'parent': '84484d20-2bd6-472d-91a1-a9c10d1bb17d', 'child': '9dbcb0cb-255c-4aca-95d0-4aaa1e199b07'}]}
